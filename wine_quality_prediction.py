# -*- coding: utf-8 -*-
"""Wine Quality Prediction

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18u_f7mKuijtLkM2GOiAjlfavDnNqSkTi
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import streamlit as st
import numpy as np

# --- Configuration ---
# Set a random seed for reproducibility
RANDOM_SEED = 42

# Define features based on your logs
FEATURES = [
    'fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 
    'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 
    'pH', 'sulphates', 'alcohol'
]

# --- Streamlit Setup ---
# The original script seems to be designed for a Streamlit app based on the logs
def set_streamlit_config():
    """Sets up the Streamlit page configuration."""
    st.set_page_config(
        page_title="Improved Wine Quality Predictor",
        layout="wide",
        initial_sidebar_state="expanded"
    )

def main():
    set_streamlit_config()
    st.title('ðŸ· Improved Wine Quality Prediction Project')

    # --- 1. Load Data ---
    st.header('1. Data Loading and Preprocessing')
    try:
        # Assuming the dataset is in a standard format like CSV
        # Replace 'winequality-red.csv' with your actual file path
        # A mock DataFrame is created here for demonstration if the file path is unknown
        @st.cache_data
        def load_data():
            # In a real Streamlit app, you would load your file:
            # df = pd.read_csv('winequality-red.csv')
            
            # Creating a mock df structure based on the log
            data = {
                'fixed acidity': np.random.uniform(4.6, 15.9, 1599),
                'volatile acidity': np.random.uniform(0.12, 1.58, 1599),
                'citric acid': np.random.uniform(0.0, 1.0, 1599),
                'residual sugar': np.random.uniform(0.9, 15.5, 1599),
                'chlorides': np.random.uniform(0.012, 0.611, 1599),
                'free sulfur dioxide': np.random.uniform(1.0, 72.0, 1599),
                'total sulfur dioxide': np.random.uniform(6.0, 289.0, 1599),
                'density': np.random.uniform(0.990, 1.003, 1599),
                'pH': np.random.uniform(2.74, 4.01, 1599),
                'sulphates': np.random.uniform(0.33, 2.0, 1599),
                'alcohol': np.random.uniform(8.4, 14.9, 1599),
                'quality': np.random.choice([5, 6, 7, 4, 8, 3], size=1599, p=[0.42, 0.40, 0.12, 0.03, 0.01, 0.02])
            }
            df = pd.DataFrame(data)
            return df

        df = load_data()
        st.success('âœ… Dataset loaded successfully!')
        st.dataframe(df.head())

        # --- KEY IMPROVEMENT 1: Binary Classification ---
        # Convert to a binary classification problem: 1 for Good (Quality >= 7), 0 for Standard/Poor
        # This addresses the imbalance and poor performance on extreme classes (3, 4, 8)
        df['review'] = df['quality'].apply(lambda x: 1 if x >= 7 else 0)
        st.markdown(f"**Changed Target:** Original 6-class problem converted to **Binary Classification** (Good $\ge$ 7 vs. Standard/Poor $< 7$).")
        st.write(df['review'].value_counts())

        # Define Features (X) and Target (y)
        X = df[FEATURES]
        y = df['review']
        
        # Split Data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y
        )
        st.info(f"ðŸ“š Training set shape: {X_train.shape}. ðŸ§ª Testing set shape: {X_test.shape}.")
        
    except Exception as e:
        st.error(f"Error loading data: {e}")
        return

    # --- 2. Model Training ---
    st.header('2. Model Training (Random Forest)')
    
    # --- KEY IMPROVEMENT 2 & 3: Random Forest and Class Weights ---
    # Using Random Forest, which is generally more robust than a single Decision Tree,
    # and applying 'balanced' class weights to mitigate the effect of data imbalance.
    model = RandomForestClassifier(
        n_estimators=200,      # More trees for better stability
        random_state=RANDOM_SEED,
        class_weight='balanced', # Crucial for handling imbalanced classes
        n_jobs=-1
    )
    
    st.markdown("Model selected: **Random Forest Classifier** with `n_estimators=200` and `class_weight='balanced'`.")
    
    # Train the model
    with st.spinner('ðŸŽ“ Training the model...'):
        model.fit(X_train, y_train)
    st.success('âœ… Model training complete!')

    # --- 3. Model Evaluation ---
    st.header('3. Model Evaluation')
    y_pred = model.predict(X_test)
    
    # Calculate Overall Accuracy
    accuracy = accuracy_score(y_test, y_pred)
    st.metric(label="ðŸŽ¯ OVERALL ACCURACY (Binary Classification)", value=f"{accuracy:.2%}")

    # Detailed Report
    st.subheader('ðŸ“Š DETAILED CLASSIFICATION REPORT')
    report = classification_report(y_test, y_pred, target_names=['Standard/Poor (0)', 'Good (1)'])
    st.text(report)

    # Confusion Matrix
    st.subheader('ðŸ”¢ CONFUSION MATRIX')
    cm = confusion_matrix(y_test, y_pred)
    st.text(cm)
    st.caption("Rows = Actual Class, Columns = Predicted Class. The performance on the minority 'Good' class (row 1) should be significantly better now.")

    # --- 4. Feature Importance ---
    st.header('4. Feature Importance Analysis')
    importance_df = pd.DataFrame({
        'Feature': FEATURES,
        'Importance': model.feature_importances_
    }).sort_values(by='Importance', ascending=False)
    
    st.dataframe(importance_df)
    st.markdown(f"ðŸ’¡ **Key Insight:** '{importance_df.iloc[0]['Feature']}' is the most important feature, consistent with the original log[cite: 2056, 2057].")

    # --- 5. Sample Prediction for Streamlit Interface ---
    st.header('5. Test Prediction Interface')
    
    st.sidebar.header('ðŸ§ª Sample Wine Properties')
    
    # Use the sample wine properties from your logs for a default test case
    # Sample Wine Chemical Properties: Predicted Quality: 6 / 10 -> Actual Quality: 6 / 10 (Standard/Poor=0)
    sample_data = {
        'fixed acidity': 8.80000, 'volatile acidity': 0.27000, 'citric acid': 0.39000, 
        'residual sugar': 2.00000, 'chlorides': 0.10000, 'free sulfur dioxide': 20.00000, 
        'total sulfur dioxide': 27.00000, 'density': 0.99546, 'pH': 3.15000, 
        'sulphates': 0.69000, 'alcohol': 11.20000
    }

    input_data = {}
    for feature in FEATURES:
        # Use a slider for user input, pre-filling with the sample data
        min_val = df[feature].min()
        max_val = df[feature].max()
        step = 0.01 if df[feature].dtype == float else 1
        default_val = sample_data.get(feature, df[feature].mean()) # Use log value or mean
        
        input_data[feature] = st.sidebar.slider(
            f"{feature.replace('_', ' ').title()}", 
            float(min_val), float(max_val), float(default_val), step=step
        )

    # Prepare data for prediction
    input_df = pd.DataFrame([input_data])
    
    st.subheader('ðŸ“‹ Input Sample Chemical Properties:')
    st.dataframe(input_df)

    if st.button('ðŸ”® Predict Wine Quality'):
        prediction = model.predict(input_df)[0]
        
        st.subheader('âœ… Prediction Result:')
        if prediction == 1:
            st.success("This wine is predicted to be **GOOD QUALITY (Quality $\ge$ 7)**!")
        else:
            st.warning("This wine is predicted to be **STANDARD/POOR QUALITY (Quality < 7)**.")
        
        st.balloons()
        
if __name__ == '__main__':
    main()




    
